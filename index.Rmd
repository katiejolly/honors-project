---
date: "`r format(Sys.time(), '%d %B, %Y')`"
title: "Katie Jolly // MSCS Honors 2019"
output: 
  html_document:
    css: styles.css
    theme: "cosmo"
---

```{r include = FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
noble_geo <- read_csv("../R/data/geocoded/noble_geo.csv")
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 2, fig.height = 2, fig.align = "center")
```


# Creating an open-source precinct shapefile for Ohio {.tabset}


This site is a culmination and explanation of the work that went into this project. On the first page you will find a brief overview of the different parts of the project. On subsequent pages, each part will be explained in more detail with images and examples.

## Overview

<br>
<br>

<img width="100%" align="middle" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\ohio-counties.png"/>


## Motivation <br>for the project

<img width="100%" align="middle" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\ohio-trial.PNG"/>



### Current status of voting rights in America




### Why are precincts important for voting rights?

Precincts are the smallest unit at which election data are reported. Counties can also provide a coarse picture of election returns, but precincts give significantly more detail to the story.	In order to make nuanced arguments about the fairness of districts, we need to know about the precinct boundaries (Geo-Enabled Elections, 2018).  

In the United States, precinct boundaries are left up to the county to decide. On the surface that is not a bad thing, but a problem arises when there is no central database for that information. Often the information isn’t digitized in an easily shareable way. In my ideal world there would be a centralized repository of shapefiles that is required to be updated whenever there is a boundary change. However, we are pretty far from that in terms of data capacity, and I realize that’s a larger problem than just precinct boundaries. 

Recently in the news, with the election and decennial census approaching in 2020, there has been a resurgent interest in polling place accessibility, voter registration campaigns, and (sometimes hidden) voter disenfranchisement. For example, there’s an ACLU lawsuit in Georgia now over closing polling places in predominantly African-American areas.The Supreme Court also recently upheld Ohio’s practice of purging inactive voters from the voter rolls (Liptak, 2018). Many of the most pressing voting rights challenges require precise knowledge of where polling places are and who is using those polling places, as well as which voters are affected by new policies.



<img width="100%" align="middle" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\nc2020.PNG"/>

This project aims to make it easier for community members to learn more about their own precincts and create one way for people to support concerns about equity and fairness with numbers. We focus on Ohio, but we hope our methodology can be repeated in other states as well and support the work of groups working on similar research. 


## Collecting publicly <br> available data 

<br>
<br>

### Related posts

[Map digitizing guide](http://www.katiejolly.io/portfolio/digitizing-guide/)

### Setting up the data collection

we started with a list of the 88 counties in Ohio and a group of about 10 fellows. For each county we looked online for a publicly available shapefile. Around 30 had shapefiles up-front. Counties fell into one of four categories: had a shapefile, had a web map, had a PDF map, had nothing available (at the time). 
After the initial internet search, we called the county Board of Elections and/or a secondary county official (a GIS specialist, for example). Our conversations with them depended on what information we had available to us already. For counties with shapefiles, we asked when that particular plan was adopted and if they had access to any previous plans’ shapefiles. For counties with a web map as the best option, which was fairly uncommon, they were almost always able to send the underlying shapefile, as well as answer all of the usual shapefile questions. Counties that had only PDF maps available online either said that the PDF was the best and only option or had a shapefile that just had not been released online, often due to a lack of strong GIS infrastructure. 


The last and most difficult category were those counties that had nothing available online. Those calls went one of three ways. First, they would be able to send us a PDF or shapefile, which was the best situation. Second, and more likely, they were able to mail us a hard copy of a map. These were often paper maps or highway maps that had precinct boundaries drawn by hand with markers. Third, and slightly less likely, they really had no maps they were able to give us. These counties were generally small and rural, although some were surprisingly populated. Fourth, the county officials never answered the phone after repeated attempts or hung up on us. That only happened in one or two counties, though. 
The counties that had no maps were the interesting ones.I will discuss more in depth how we handled drawing precincts for those counties in the [Approximating precinct maps section](#approximating-precinct-maps). Ultimately we used geocoding and the addresses listed in the voterfile to approximate the boundaries.


In total, we were given shapefiles from 50 counties, PDF or paper maps from 31 counties, and no maps from 7 counties. The next few sections describe our methodology for each of the cases. Note that for counties that provided a shapefile, often the only work on our part was creating a projection file when one was not already included so I do not discuss that methodology in this paper. 


### Digitizing maps

In this section I refer to digitizing as the full process including both georeferencing and vectorizing.
Among counties that provided us with PDF or paper maps, there was a wide range of quality and quantity. Some counties had a single PDF map of all the precincts whereas other counties had one precinct per PDF, which could be as many as 100 or so. Other counties mailed us paper maps, which also had a wide range of quality. There were even counties that split precincts among several PDFs. To digitize we used OpenStreetMap in QGIS. 

There were approximately 450 total images to digitize. We hosted “digitizing workshops” and invited everyone at the research institute to attend with the promise of a free dinner and new technical skills. At the first one, I led a short lesson on georeferencing and map projection. We also made a step-by-step guide for georeferencing that was specific to these maps. Some images from the guide are shown below.

<img src="images/digitizing_collage.png" alt="Collage of pages from our book on digitizing. Includes information about projection, adding images, checking data, and others." width="70%" height = "70%" align="center">

During the second workshop we started vectorizing the georeferenced maps. This was prone to a few more technical issues, including difficult angles and fine-tuning. Due to time constraints we vectorized by selecting census blocks that were within the precincts because in most places, precinct lines followed census blocks. From a project management standpoint we usually assigned the georeferencing of a county to a group of two to four people and then assigned only one person for the vectorizing. This helped minimize opportunities for human error. 


In total we estimated that we spent about 400 person hours on the project. Some of that time comes from the fact that many people had never used GIS before and we had to redo a few counties. In general, it’s a long and tedious process. As of now, by hand seems to be the only viable option. However, in the next section I will discuss an algorithm that was originally designed to impute missing data but the hope is that we could automate some of the digitizing in the future as well. 









## Approximating <br>precinct maps

<br>

### Related posts

[Writing a classification algorithm to build precinct shapefiles in Ohio](http://www.katiejolly.io/blog/2018-10-04/ohio-precinct-classification)

[Evaluating our precinct boundary approximation algorithm with v-measure](http://www.katiejolly.io/blog/2018-10-27/vmeasure-ohio)

<br> <br>

Most of the counties we worked with in Ohio had either a shapefile available or pdf maps that we could digitize. The more challenging case was when no map existed that we could use. Much of the research I did was related to how to use other publicly available data to approximate the precinct boundaries in these counties. There were about ten counties total in this category. In the end we found that geocoded addresses from the [publicly available voter file](https://www6.sos.state.oh.us/ords/f?p=111:1) could draw “accurate-enough” precinct boundaries in the absence of a better option. Groups like the National States Geographic Information Council are also using voter addresses to geocode precinct boundaries with address range shapefiles. The general approach we settled on was to use the  to find the locations of currently registered voters and then triangulate plausible precinct boundaries using a nearest neighbor algorithm. The details of this algorithm are below. To illustrate the algorithm, we use Noble County, Ohio as an example. 

### Data

The first consideration is the readily available raw data that is available to researchers and community members. Calling counties for more data is an option, though a time consuming and tedious process. The ultimate goal is to make phone calls less necessary for obtaining data. 

As previously mentioned, the voter files in Ohio (a list of registered voters) is publicly available online. The files are compiled by county, so in total there are 88 voter files in Ohio. Our variables of interest are the address field and the assigned precinct. An example of the voter file for Noble county is shown below:

```{r echo = FALSE}
kable(noble_geo %>% select(RESIDENTIAL_ADDRESS1, RESIDENTIAL_CITY, RESIDENTIAL_STATE, RESIDENTIAL_ZIP, PRECINCT_NAME) %>% head()) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, font_size = 10)
```

The other piece of necessary data is an underlying geographic unit for building the precincts. The natural choice is census blocks because in theory precinct boundaries do not split census blocks. Additionally, the method of digitizing we implemented uses census blocks to fill in precinct polygons Census blocks are the smallest geographic unit defined by the Census Bureau. In urban areas, census blocks are often analagous to city blocks, but this analogy does not hold in rural areas. The tigris (**cite!!**) package in R to accesses the census block shapefiles published by the Census Bureau (**cite!!**) for us. 

To connect the voter file with the census blocks, we make use of a Census Bureau API through the tigris package in R (**cite!!**) that returns a census block ID for each valid address. In general, we find that 60 to 80 percent of the addresses are valid, meaning they can be processed by the API. Below on the left is a map highlighting the census blocks with no valid addresses and on the right is a map showing how many addresses are valid per census block. 

<br>
<br>

<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\R\\plots\\valid_addresses.png"/>
<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\R\\plots\\noble1.png"/>

For the blocks with valid addresses, there is information in the voter file about what precinct that address belongs in. We define the precinct of a block with valid addresses to be the most common precinct of the listed addresses. Often these are unanimous, but in scenarios with data entry errors, split blocks, or other irregularities there may be multiple precincts listed in a block. 

This leaves slightly fewer than half of the census blocks unclassified in most counties. To fill in the gaps, we wrote an algorithm to impute the values using a nearest neighbor model.

### Defining a model

The idea behind our approximation method is to use nonparametric classification to impute the missing block precinct assignments. The initial idea comes from Tobler's first law of geography, "everything is related to everything else, but near things are more related than distant things." In this context, adjacent blocks are more related than non-adjacent blocks in terms of precinct assignment. This is true conceptually, but also many states require precincts to be contiguous polygons. We can formalize this idea with a nearest neighbor model that depends on k neighbors, where k is a function of the particular block. 

We define this model as a nearest neighbor model that depends on the number of adjacent neighbors of a given block $x_i$, of $I$ total blocks in a county. The general framework is similar to a $knn$ model, where $k$ = the size of the neighborhood. The difference is that $k$ varies by block $x_i$ in this case, so that parameter is instead $k(x_i)$. Different blocks have different numbers of adjacent neighbors, thus thus the neighborhood size is a function of the block. 

From our set of $L$ possible precincts, the predicted precinct $\hat{p}_{k(x_i)nn}$ of a block $x_i$ is the most common precinct in its neighborhood of size $k(x_i)$, where $y_j$ is the precinct assignment of the $jth$ neighbor. The number of neighboring blocks in precinct $p_l$ is defined by $N_l(x_i)$. Ties for the most common precinct are broken at random.

$$\hat{p}_{k(x_i)nn} \in \text{argmax }N_l(x_i), \quad l \in \{1, 2, 3, ... L\}$$ 

$$\text{where } N_l(x_i):= \sum_{j=1}^{k(x_i)}\mathbf{1}(y_j=p_l)$$

After we run this model, there will still be unclassified blocks. This comes from the fact that not every unclassified block has classified neighbors. This then becomes an iterative model. The process is repeated until every block is classified. Blocks classified in previous iterations become part of the training set, so each iteration has more training data than previous ones. 

If we wanted a probabilistic model instead of this deterministic one, we could define the probability of block $x_i$ being in precinct $p_l$ as the fraction of its neighbors in that precinct. 

As an improvement to the original model definition, we include a requirement that a minimum fraction of a block's neighborhood, $\alpha$ must be classified in order to make a classification decision. This condition is supplied by the user. In this case, our model becomes:

$$\textbf{if: }\quad \frac{\sum_j^{k(x_i)}\mathbf{1} (y_j = NULL)}{k(x_i)} \geq \alpha$$

$$\hat{p}_{k(x_i)nn} \in \text{argmax }N_l(x_i), \quad l \in \{1, 2, 3, ... L\}$$ 

$$\text{where } N_l(x_i):= \sum_{j=1}^{k(x_i)}\mathbf{1}(y_j=p_l)$$

$$\textbf{else: } \quad \hat{p}_{k(x_i)nn} = NULL$$

This ensures that spurious patterns from classifying with little data can be mitigated. Adding a minimum value that is too high prevents the algorithm from running to completion in some cases, so finding a balance is often a process of trial and error.

In the next section, I will discuss some techniques for finding a balance between the strictness of our algorithm and the efficiency of the algorithm. 




### Evaluating the model

One question that we had was how robust the algorithm was to changes to the minimum required neighbors. We tested $\alpha \in \{0, 0.25, 0.35, 0.4, 0.5, 0.8\}$. 

In general we found that $\alpha > 0.25$ was too strict in this particular county. There would be a point at which there were no unclassified blocks that met the requirements and the percent classified would reach a horizontal asymptote (see figure below). However, in other counties the "maximum minimum" proportion could be different. The best method is to try a variety in each context. 

<img width="100%" align = "middle"  src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_proportions.PNG"/>

We can also look at this process in detail for certain minimum proportions. Below are some maps that help detail this process.

**right now only the minimum = 0 maps are included**

**eventually I want the color scheme to match up with the plot above for all of the minimum values to make comparison easier**

<img width="55%" align="middle" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_0_initial.png"/>

<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_0_1.png"/>
<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_0_2.png"/>

<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_0_3.png"/>
<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_0_4.png"/>

<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_0_5.png"/>
<img width="45%" src="C:\\Users\\katie\\Documents\\honors\\paper\\images\\noble_0_6.png"/>




After defining and implementing a model for imputing precinct names, we want to have a method for evaluating these maps. Two factors make this more complex. First, we do not have true values for the counties that did not provide data. Second, many model assessments are built for a quantitative response. We also needed an assessment built specifically for spatial qualitative response variables.

In order to have "true" values for comparing the approximated ones, we independently generated precinct maps for counties that did provide data. Using multiple counties allowed us to have an idea of the spread of possible accuracies. To compare the maps we use a spatial validity measure (**cite!!**), or v-measure. This measures the “sameness” of the different precinct labels, or how well two different precinct maps fit together in one domain (a county). For simplicity, we refer to the official precinct boundaries as a regionalization and the approximate boundaries as a partition (and the individual units of the partition are the zones). We compute values for completeness, the average “sameness” of the regions with respect to zones, and homogeneity, the average “sameness” of zones with respect to regions. The v-measure is a harmonic mean of completeness and homogeneity and ranges between 0 and 1, with 1 being a perfect match. The methodology for v-measure is defined below. 

We will call the county, our area of interest, $A$. The regionalization $R$, the official precincts, divides $A$ into $n$ regions $r_z | z = 1, 2, 3, ..., n$. The partition $W$, our approximated precincts, divides $A$ into $m$ zones $w_y | 1, 2, 3, ..., m$. We superimpose the regionalization and partition to subdivide the domain into $m x n$ units, having areas $a_{zy} | z = 1, ..., n; y = 1, ..., m$. In this case, $a_{zy}$ is the area of the intersection of region $z$ and zone $y$. 

**continue writing the rest of the process here!!** 



The paper on using v-measure for spatial data by Nowosad and Stepinski is [freely available online](https://eartharxiv.org/rcjh7/).

To demostrate how we use v-measure, I will use Clark County as an example.For this county, we were given official precincts by the county. We also generated our own approximations using our model. 

**insert side-by-side maps here!!** 

Counties like Clark help validate our method, by telling us how closely our approximations match the actual map. It is not necessarily a definitive measure, but it can be used to give a general idea of model adequacy. 

Intitially we implemented a precinct approximation model defined by $\alpha = 0$, meaning only one neighbor has to be classified to classify a block. When we calculated v-measure, we got an overall score of $0.97$. **check overall and completeness/homogeneity values**. V-measure also calculates local inhomogeneity, which gives an idea of exactly which $a_{zy}$ units are the most inhomogeneous, or poorly approximated according to this measure. 

**insert local inhomogeneity map here!!**

When we look at this map, we see a few spatial patterns. First, we see a line of inhomogeneity through the middle of the county. From outside research we know that this line tracks with a major highway. 

**insert map with arrow?**

The spatial autocorrelation of our errors suggests latent patterns that our model cannot account for. Being able to identify covariates, such as the presence of the highway, adds to the information we have about what those latent variables might be. In this case, we know from prior knowledge that precinct boundaries often follow major roads or municipal boundaries. Through the census we can access this data. In future versions of the algorithm, we hope to include mechanisms for accounting for updating the probability that blocks are in the same precinct according to this outside information about other spatial patterns. 


### Suggested workflow

After the release of our upcoming R package `{approxprecincts}`, anyone will be able to implement the approximation algorithm. We currently do not implement v-measure code in our package, because the `{sabre}` package already does that well. Below I will demonstrate the general workflow for any Ohio county. Take note that the geocoding function speed is highly dependent on the number of voters registered in a county. We are yet to test how well random samples of voter addresses perform relative to the whole set, but that is something worth exploring for the sake of efficiency and robustness.

This example is for Noble County, but the process is similar for any Ohio county.

We start by initially just creating the first iteration.

```{r eval = FALSE, include = TRUE, echo = TRUE}
county_voterfile <- load_voterfile(county = "Noble")

county_blocks <- load_blocks(county = "Noble")

voterfile_geo <- geocode_voterfile(county_voterfile)

iter_1 <- voterfile_geo %>%
  join_voters_to_blocks() %>% # matches up voters with census blocks based on the geocoding process
  find_neighbors(type = "rook") %>% # creates a vector of neighbors for each block, can be rook or queen adjacency
  lookup_precincts_nn() %>% # creates a vector of precinct names with a name for each neighbor
  classify_nn(alpha = 0) # finds the most common precinct name, if at least the minimum proportion is met
```

After the first iteration is complete, we no longer need to look up the neighbors of each block anymore. We only need to start the process with updating the vector of precinct names to update our information about the map. 

```{r eval = FALSE, include = TRUE, echo = TRUE}

iter_2 <- iter_1 %>%
  lookup_precincts_nn() %>%
  classify_nn()
```

We repeat this process until all the blocks are classified. The function will print a message telling the user how many blocks are left after each iteration, to give an idea of performance over time. 

### A reproducible process

If you would like to reproduce these maps, or produce them for your own work, we have compiled this algorithm into an R package. You can install it from the [GitHub repository](https://github.com/ykelly/approxprecincts) as it is not available on CRAN. 

## What can we <br>do with this?

## More resources

<br>
<br>

### Related posts 

[GIS/LIS Conference Presentation Resource Page](http://www.katiejolly.io/honors-project-ohio/)

<br>
<br>

This group is certainly not the only one working on voting rights and data science for social good. Here is a starting list of other organizations to know about if you're interested in this work!

...

...

...